{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package ml-feature (org.apache.spark.ml.feature)\n",
    "\n",
    "Il s'agit du package de feature engineering (préparation et formatage de données). Il est divisé en trois parties :\n",
    "\n",
    "* **Transformation :** traitement de variables ; normalisation, standardisation, codification, ...\n",
    "* **Extraction :** Extraction de variables à partir de données \"brutes\".\n",
    "* **Sélection :** Sélection de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Représentation des vecteurs avec Spark\n",
    "\n",
    "Les vecteurs denses et sparses constituent l'un élément de base de Spark ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs denses stockent l’intégralité des valeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.linalg.Vectors\r\n",
       "vectDense: org.apache.spark.ml.linalg.Vector = [1.0,1.3,0.0,3.0,3.5,5.0]\r\n",
       "vectZeros: org.apache.spark.ml.linalg.Vector = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "// Créer un vecteur dense (1.5, 0.0, 3.5)\n",
    "val vectDense = Vectors.dense(1, 1.3, 0.0, 3, 3.5, 5)\n",
    "\n",
    "// Créer un vecteur dense de dimension 10 avec toutes les composantes 0.0\n",
    "val vectZeros = Vectors.zeros(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les vecteurs sparses ou vecteurs creux stockent seulement les valeurs non-nulles et leurs indices. On peut créer un vecteur sparse de deux façons :\n",
    "\n",
    "* `Vectors.sparse(size, indices, values)`\n",
    "* `Vectors.sparse(size, Seq(indices, values))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vectSparse1: org.apache.spark.ml.linalg.Vector = (5,[1,3,4],[10.5,3.0,11.0])\r\n",
       "vectSparse2: org.apache.spark.ml.linalg.Vector = (5,[1,3,4],[10.5,3.0,11.0])\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// En indiquant les indices des composantes nulles (1, 3, 4) et leur valeur\n",
    "// vectSparse = (0.0, 10.5, 0.0, 3.0, 11.0), \n",
    "val vectSparse1 = Vectors.sparse(5, Array(1, 3, 4), Array(10.5, 3.0, 11.0))\n",
    "\n",
    "// En indiquant la séquence des paires (indice, valeur) pour les composantes non nulles\n",
    "val vectSparse2 = Vectors.sparse(5, Seq((1, 10.5), (3, 3.0), (4, 11.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** : \n",
    "\n",
    "Scala a sa propre représentation des vecteurs avec `scala.collection.immutable.Vector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les transformateurs (Feature Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorAssembler** : regroupe des colonnes en une seule colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-----------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 3 more fields]\r\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_95d0478000a4, handleInvalid=error, numInputCols=4\r\n",
       "dfmAssembled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val dfm = spark.read.option(\"header\", true).option(\"inferSchema\", \"true\").csv(\"../data/iris.txt\")\n",
    "dfm.show(2)\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val dfmAssembled = assembler.transform(dfm)\n",
    "\n",
    "dfmAssembled.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorIndexer** : identifier des variables catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-----------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Chose 0 categorical features: \n",
      "+-------+-------+-------+-------+-----------+-----------------+-----------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|          indexed|\n",
      "+-------+-------+-------+-------+-----------+-----------------+-----------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|[5.1,3.5,1.4,0.2]|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|[4.9,3.0,1.4,0.2]|\n",
      "+-------+-------+-------+-------+-----------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorIndexer\r\n",
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 3 more fields]\r\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_a243cc922027, handleInvalid=error, numInputCols=4\r\n",
       "dfmAssembled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 4 more fields]\r\n",
       "indexer: org.apache.spark.ml.feature.VectorIndexer = vecIdx_70b50df2e535\r\n",
       "indexerModel: org.apache.spark.ml.feature.VectorIndexerModel = VectorIndexerModel: uid=vecIdx_70b50df2e535, numFeatures=4, handleInvalid=error\r\n",
       "categoricalFeatures: Set[Int] = Set()\r\n",
       "indexedData: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorIndexer\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "val dfm = spark.read.option(\"header\", true).option(\"inferSchema\", \"true\").csv(\"../data/iris.txt\")\n",
    "dfm.show(2)\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val dfmAssembled = assembler.transform(dfm)\n",
    "\n",
    "dfmAssembled.show(2)\n",
    "\n",
    "val indexer = new VectorIndexer()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"indexed\")\n",
    "  .setMaxCategories(4)\n",
    "\n",
    "val indexerModel = indexer.fit(dfmAssembled)\n",
    "\n",
    "val categoricalFeatures: Set[Int] = indexerModel.categoryMaps.keys.toSet\n",
    "println(s\"Chose ${categoricalFeatures.size} \" +\n",
    "  s\"categorical features: ${categoricalFeatures.mkString(\", \")}\")\n",
    "\n",
    "// Create new column \"indexed\" with categorical values transformed to indices\n",
    "val indexedData = indexerModel.transform(dfmAssembled)\n",
    "indexedData.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StringIndexer** : permet de créer une variable numérique à partir d'une variable catégorielle en remplaçant les modalités par des valeurs numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|    mod1|          0.0|\n",
      "|  1|    mod2|          1.0|\n",
      "|  2|    mod3|          2.0|\n",
      "|  3|    mod1|          0.0|\n",
      "|  4|    mod1|          0.0|\n",
      "|  5|    mod2|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- category: string (nullable = true)\n",
      " |-- categoryIndex: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.StringIndexer\r\n",
       "sq: Seq[(Int, String)] = List((0,mod1), (1,mod2), (2,mod3), (3,mod1), (4,mod1), (5,mod2))\r\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, category: string]\r\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_af1765017d7b\r\n",
       "indexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "\n",
    "val sq = Seq((0, \"mod1\"), (1, \"mod2\"), (2, \"mod3\"), (3, \"mod1\"), (4, \"mod1\"), (5, \"mod2\"))\n",
    "\n",
    "val df = spark.createDataFrame(sq).toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"category\")\n",
    "  .setOutputCol(\"categoryIndex\")\n",
    "\n",
    "val indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()\n",
    "\n",
    "indexed.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IndexToString** : réalise l'opération inverse de StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+----------------+\n",
      "|id |category|categoryIndex|originalCategory|\n",
      "+---+--------+-------------+----------------+\n",
      "|0  |mod1    |0.0          |mod1            |\n",
      "|1  |mod2    |1.0          |mod2            |\n",
      "|2  |mod3    |2.0          |mod3            |\n",
      "|3  |mod1    |0.0          |mod1            |\n",
      "|4  |mod1    |0.0          |mod1            |\n",
      "|5  |mod2    |1.0          |mod2            |\n",
      "+---+--------+-------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{StringIndexer, IndexToString}\r\n",
       "sq: Seq[(Int, String)] = List((0,mod1), (1,mod2), (2,mod3), (3,mod1), (4,mod1), (5,mod2))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [id: int, category: string]\r\n",
       "indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_bf07832d19e1\r\n",
       "indexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\r\n",
       "converter: org.apache.spark.ml.feature.IndexToString = idxToStr_891d29052332\r\n",
       "converted: org.apache.spark.sql.DataFrame = [id: int, category: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString}\n",
    "\n",
    "val sq = Seq((0, \"mod1\"), (1, \"mod2\"), (2, \"mod3\"), (3, \"mod1\"), (4, \"mod1\"), (5, \"mod2\"))\n",
    "val dfm = spark.createDataFrame(sq). toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "                  .setInputCol(\"category\")\n",
    "                  .setOutputCol(\"categoryIndex\")\n",
    "\n",
    "val indexed = indexer.fit(dfm).transform(dfm)\n",
    "\n",
    "val converter = new IndexToString()\n",
    "                   .setInputCol(\"categoryIndex\")\n",
    "                    .setOutputCol(\"originalCategory\")\n",
    "val converted = converter.transform(indexed)\n",
    "\n",
    "converted.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OneHotEncoder :** crée une variable binaire pour chaque modalité d'une variable catégorielle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+-------------+\n",
      "| id|category|categoryIndex|  categoryVec|\n",
      "+---+--------+-------------+-------------+\n",
      "|  0|    mod1|          0.0|(2,[0],[1.0])|\n",
      "|  1|    mod2|          1.0|(2,[1],[1.0])|\n",
      "|  2|    mod3|          2.0|    (2,[],[])|\n",
      "|  3|    mod1|          0.0|(2,[0],[1.0])|\n",
      "|  4|    mod1|          0.0|(2,[0],[1.0])|\n",
      "|  5|    mod2|          1.0|(2,[1],[1.0])|\n",
      "+---+--------+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.OneHotEncoder\r\n",
       "sq: Seq[(Int, String)] = List((0,mod1), (1,mod2), (2,mod3), (3,mod1), (4,mod1), (5,mod2))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [id: int, category: string]\r\n",
       "indexer: org.apache.spark.ml.feature.StringIndexerModel = StringIndexerModel: uid=strIdx_af37b1408f98, handleInvalid=error\r\n",
       "indexed: org.apache.spark.sql.DataFrame = [id: int, category: string ... 1 more field]\r\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderModel = OneHotEncoderModel: uid=oneHotEncoder_3098098b56f7, dropLast=true, handleInvalid=error\r\n",
       "encoded: org.apache.spark.sql.DataFrame = [id: int, category: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{OneHotEncoder}\n",
    "\n",
    "val sq = Seq((0, \"mod1\"), (1, \"mod2\"), (2, \"mod3\"), (3, \"mod1\"), (4, \"mod1\"), (5, \"mod2\"))\n",
    "\n",
    "val dfm = spark.createDataFrame(sq).toDF(\"id\", \"category\")\n",
    "\n",
    "val indexer = new StringIndexer()\n",
    "                 .setInputCol(\"category\")\n",
    "                 .setOutputCol(\"categoryIndex\").fit(dfm)\n",
    "\n",
    "val indexed = indexer.transform(dfm) \n",
    "\n",
    "val encoder = new OneHotEncoder()\n",
    "                 .setInputCol(\"categoryIndex\")\n",
    "                 .setOutputCol(\"categoryVec\")\n",
    "                 .fit(indexed)\n",
    "val encoded = encoder.transform(indexed)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizer** : normaliser les données en utilisant la norme Lp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-----------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-------+-------+-----------+-----------------+--------------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|        normFeatures|\n",
      "+-------+-------+-------+-------+-----------+-----------------+--------------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|[0.80377277301538...|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|[0.82813287338687...|\n",
      "+-------+-------+-------+-------+-----------+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.feature.Normalizer\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 3 more fields]\r\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_8de23667a13c, handleInvalid=error, numInputCols=4\r\n",
       "dfmAssembled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 4 more fields]\r\n",
       "normalizer: org.apache.spark.ml.feature.Normalizer = Normalizer: uid=normalizer_caae2a61f658, p=2.0\r\n",
       "dfmNormalized: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val dfm = spark.read.option(\"header\", true).option(\"inferSchema\", \"true\").csv(\"../data/iris.txt\")\n",
    "dfm.show(2)\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val dfmAssembled = assembler.transform(dfm)\n",
    "\n",
    "dfmAssembled.show(2)\n",
    "\n",
    "val normalizer = new Normalizer()\n",
    "                  .setInputCol(\"features\") \n",
    "                  .setOutputCol(\"normFeatures\") \n",
    "                  .setP(2.0) \n",
    "val dfmNormalized = normalizer.transform(dfmAssembled)\n",
    "dfmNormalized.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StandardScaler** : Standardiser des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-----------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|\n",
      "+-------+-------+-------+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|\n",
      "+-------+-------+-------+-------+-----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-------+-------+-------+-----------+-----------------+--------------------+\n",
      "|sepal_l|sepal_w|petal_l|petal_w|     classe|         features|      scaledFeatures|\n",
      "+-------+-------+-------+-------+-----------+-----------------+--------------------+\n",
      "|    5.1|    3.5|    1.4|    0.2|Iris-setosa|[5.1,3.5,1.4,0.2]|[-0.8976738791967...|\n",
      "|    4.9|    3.0|    1.4|    0.2|Iris-setosa|[4.9,3.0,1.4,0.2]|[-1.1392004834649...|\n",
      "+-------+-------+-------+-------+-----------+-----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.feature.StandardScaler\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 3 more fields]\r\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_4ef06c3c7661, handleInvalid=error, numInputCols=4\r\n",
       "dfmAssembled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 4 more fields]\r\n",
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_25baa67de171\r\n",
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = StandardScalerModel: uid=stdScal_25baa67de171, numFeatures=4, withMean=true, withStd=true\r\n",
       "dfmScaled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val dfm = spark.read.option(\"header\", true).option(\"inferSchema\", \"true\").csv(\"../data/iris.txt\")\n",
    "dfm.show(2)\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val dfmAssembled = assembler.transform(dfm)\n",
    "\n",
    "dfmAssembled.show(2)\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(true)\n",
    "\n",
    "val scalerModel = scaler.fit(dfmAssembled)\n",
    "\n",
    "val dfmScaled = scalerModel.transform(dfmAssembled)\n",
    "dfmScaled.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer** : transformer une phrase ou une ligne en un vecteur de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(je, veux, aller, au, cinema.),0]\n",
      "[WrappedArray(le, train, est, en, retard.),1]\n",
      "[WrappedArray(la, maison, est, tres, belle.),2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Tokenizer\r\n",
       "dfmPhrase: org.apache.spark.sql.DataFrame = [id: int, phrase: string]\r\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_1f0a6151e0bd\r\n",
       "tokenized: org.apache.spark.sql.DataFrame = [id: int, phrase: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val dfmPhrase= spark.createDataFrame(Seq(\n",
    "            (0, \"Je veux aller au cinema.\"),\n",
    "            (1, \"Le train est en retard.\"),\n",
    "            (2, \"La maison est tres belle.\")\n",
    "            )).toDF(\"id\", \"phrase\")\n",
    "\n",
    "val tokenizer = new Tokenizer().setInputCol(\"phrase\").setOutputCol(\"mots\")\n",
    "\n",
    "val tokenized = tokenizer.transform(dfmPhrase)\n",
    "\n",
    "tokenized.select(\"mots\", \"id\").take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binarizer** : transformer une variable numérique en une variable binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|feature|label|\n",
      "+---+-------+-----+\n",
      "|  0|    0.1|  0.0|\n",
      "|  1|    0.3|  0.0|\n",
      "|  2|    0.9|  1.0|\n",
      "|  3|    0.8|  1.0|\n",
      "|  0|    0.0|  0.0|\n",
      "|  0|    0.6|  1.0|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Binarizer\r\n",
       "array: Array[(Int, Double)] = Array((0,0.1), (1,0.3), (2,0.9), (3,0.8), (0,0.0), (0,0.6))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [id: int, feature: double]\r\n",
       "binarizer: org.apache.spark.ml.feature.Binarizer = Binarizer: uid=binarizer_57d2f08f5512\r\n",
       "dfmBinarized: org.apache.spark.sql.DataFrame = [id: int, feature: double ... 1 more field]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Binarizer\n",
    "\n",
    "val array = Array((0, 0.1), (1,  0.3), (2, 0.9), (3, 0.8),\n",
    "                  (0, 0.0), (0, 0.6))\n",
    "val dfm = spark.createDataFrame(array).toDF(\"id\", \"feature\")\n",
    "\n",
    "val binarizer = new Binarizer()\n",
    "            .setInputCol(\"feature\")\n",
    "            .setOutputCol(\"label\")\n",
    "             .setThreshold(0.5)\n",
    "\n",
    "val dfmBinarized = binarizer.transform(dfm)\n",
    "dfmBinarized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketizer ** : \n",
    "\n",
    "Bucketizer découpe une variable numérique en une variable catégorielle avec des classes. Les intervalles pour définir les classes sont fixés par l'utilisateur avec le paramètre `splits`. \n",
    "\n",
    "Par exemple une variable `age` peut découper en tranche d'âge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  0|  7|\n",
      "|  1|  5|\n",
      "+---+---+\n",
      "only showing top 2 rows\n",
      "\n",
      "Bucketizer output with 5 buckets\n",
      "+---+---+-----------+\n",
      "| id|age|bucketedAge|\n",
      "+---+---+-----------+\n",
      "|  0|  7|        3.0|\n",
      "|  1|  5|        2.0|\n",
      "|  2|  3|        1.0|\n",
      "|  3|  8|        4.0|\n",
      "|  4|  4|        2.0|\n",
      "|  5|  6|        3.0|\n",
      "|  6|  1|        0.0|\n",
      "|  7| 13|        4.0|\n",
      "+---+---+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Bucketizer\r\n",
       "splits: Array[Double] = Array(0.0, 2.0, 4.0, 6.0, 8.0, Infinity)\r\n",
       "array: Array[(Int, Int)] = Array((0,7), (1,5), (2,3), (3,8), (4,4), (5,6), (6,1), (7,13))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [id: int, age: int]\r\n",
       "bucketizer: org.apache.spark.ml.feature.Bucketizer = Bucketizer: uid=bucketizer_a0bc3b24c907\r\n",
       "bucketedData: org.apache.spark.sql.DataFrame = [id: int, age: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Bucketizer\n",
    "\n",
    "val splits = Array(0, 2, 4, 6, 8, Double.PositiveInfinity)\n",
    "\n",
    "val array = Array((0, 7), (1,  5), (2, 3), (3, 8),\n",
    "                  (4, 4), (5, 6), (6, 1), (7, 13))\n",
    "val dfm = spark.createDataFrame(array).toDF(\"id\", \"age\")\n",
    "\n",
    "dfm.show(2)\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"age\")\n",
    "  .setOutputCol(\"bucketedAge\")\n",
    "  .setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(dfm)\n",
    "\n",
    "println(s\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\")\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les extracteurs (Feature extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDF** : calcule le Inverse Document Frequency (IDF) pour une collection de documents données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|label|features                                                                             |\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "|0.0  |(10,[0,2,4,6,8],[0.0,0.6931471805599453,0.0,0.28768207245178085,0.28768207245178085])|\n",
      "|1.0  |(10,[0,3,4,7],[0.0,0.6931471805599453,0.0,0.6931471805599453])                       |\n",
      "|0.0  |(10,[0,1,4,6,8],[0.0,0.6931471805599453,0.0,0.28768207245178085,0.28768207245178085])|\n",
      "+-----+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.HashingTF\r\n",
       "import org.apache.spark.ml.feature.IDF\r\n",
       "import org.apache.spark.ml.feature.Tokenizer\r\n",
       "dfmPhrase: org.apache.spark.sql.DataFrame = [label: double, sentence: string]\r\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_da8893aebbce\r\n",
       "wordsData: org.apache.spark.sql.DataFrame = [label: double, sentence: string ... 1 more field]\r\n",
       "hashingTF: org.apache.spark.ml.feature.HashingTF = HashingTF: uid=hashingTF_8e7004cd51eb, binary=false, numFeatures=10\r\n",
       "featurizedData: org.apache.spark.sql.DataFrame = [label: double, sentence: string ... 2 more fields]\r\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_55542dac47ee\r\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = IDFModel: uid=idf_55542dac47ee, numDocs=3, numFeatures=10\r\n",
       "rescaledData: org.apache.spark.sql...\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.HashingTF\n",
    "import org.apache.spark.ml.feature.IDF\n",
    "import org.apache.spark.ml.feature.Tokenizer\n",
    "\n",
    "val dfmPhrase = spark.createDataFrame(Seq(\n",
    "            (0.0, \"Je veux aller au cinema.\"),\n",
    "            (1.0, \"Le train est en retard.\"),\n",
    "            (0.0, \"La maison est tres belle.\")\n",
    "            )).toDF(\"label\", \"sentence\")\n",
    "\n",
    "val tokenizer = new Tokenizer()\n",
    "                  .setInputCol(\"sentence\")\n",
    "                  .setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(dfmPhrase)\n",
    "\n",
    "val hashingTF = new HashingTF().setInputCol(\"words\")\n",
    "                   .setOutputCol(\"rawFeatures\")\n",
    "                   .setNumFeatures(10)\n",
    "\n",
    "val featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "val rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.select(\"label\", \"features\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec** : représente un document en un vecteur numérique de dimension fixe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              phrase|\n",
      "+--------------------+\n",
      "|[Le, train, est, ...|\n",
      "|[Le, train, entre...|\n",
      "|[Les, voyageurs, ...|\n",
      "+--------------------+\n",
      "\n",
      "Texte: [Le, train, est, en, retard] => \n",
      "Vecteur: [-0.06348072681576014,0.00417338686529547,0.03518190383911133]\n",
      "\n",
      "Texte: [Le, train, entre, en, gare] => \n",
      "Vecteur: [-0.03118861708790064,0.03171334862709046,0.029982009530067445]\n",
      "\n",
      "Texte: [Les, voyageurs, descendent, du, train] => \n",
      "Vecteur: [-0.03028365280479193,-0.02631254754960537,0.01358244437724352]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.Word2Vec\r\n",
       "import org.apache.spark.ml.linalg.Vector\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "phraseDF: org.apache.spark.sql.DataFrame = [phrase: array<string>]\r\n",
       "word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_cd91e925ea31\r\n",
       "model: org.apache.spark.ml.feature.Word2VecModel = Word2VecModel: uid=w2v_cd91e925ea31, numWords=11, vectorSize=3\r\n",
       "output: org.apache.spark.sql.DataFrame = [phrase: array<string>, features: vector]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.Word2Vec\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val phraseDF = spark.createDataFrame(Seq(\n",
    "  \"Le train est en retard\",\n",
    "  \"Le train entre en gare\",\n",
    "  \"Les voyageurs descendent du train\"\n",
    ").map(x => x.split(\" \")).map(Tuple1.apply)).toDF(\"phrase\")\n",
    "\n",
    "phraseDF.show()\n",
    "\n",
    "// Learn a mapping from words to Vectors.\n",
    "val word2Vec = new Word2Vec()\n",
    "  .setInputCol(\"phrase\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVectorSize(3)\n",
    "  .setMinCount(0)\n",
    "\n",
    "val model = word2Vec.fit(phraseDF)\n",
    "\n",
    "val output = model.transform(phraseDF)\n",
    "\n",
    "output.collect().foreach { case Row(text: Seq[_], features: Vector) =>\n",
    "  println(s\"Texte: [${text.mkString(\", \")}] => \\nVecteur: $features\\n\") }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CountVectorizer** : compte le nombre d'occurrence d'un mot dans un document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[Le, train, est, ...|\n",
      "|[Le, train, entre...|\n",
      "|[Les, voyageurs, ...|\n",
      "+--------------------+\n",
      "\n",
      "+---------------------------------------+-------------------------+\n",
      "|words                                  |features                 |\n",
      "+---------------------------------------+-------------------------+\n",
      "|[Le, train, est, en, retard]           |(5,[0,1,2],[1.0,1.0,1.0])|\n",
      "|[Le, train, entre, en, gare]           |(5,[0,1,2],[1.0,1.0,1.0])|\n",
      "|[Les, voyageurs, descendent, du, train]|(5,[0,3,4],[1.0,1.0,1.0])|\n",
      "+---------------------------------------+-------------------------+\n",
      "\n",
      "Vocabulaire = train, en, Le, Les, du\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\r\n",
       "phraseDF: org.apache.spark.sql.DataFrame = [words: array<string>]\r\n",
       "countVectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_f213f9a63710\r\n",
       "model: org.apache.spark.ml.feature.CountVectorizerModel = CountVectorizerModel: uid=cntVec_f213f9a63710, vocabularySize=5\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.CountVectorizer\n",
    "\n",
    "val phraseDF = spark.createDataFrame(Seq(\n",
    "  \"Le train est en retard\",\n",
    "  \"Le train entre en gare\",\n",
    "  \"Les voyageurs descendent du train\"\n",
    ").map(x => x.split(\" \")).map(Tuple1.apply)).toDF(\"words\")\n",
    "\n",
    "phraseDF.show()\n",
    "\n",
    "val countVectorizer = new CountVectorizer()\n",
    "  .setInputCol(\"words\")\n",
    "  .setOutputCol(\"features\")\n",
    "  .setVocabSize(5)\n",
    "  .setMinDF(0)\n",
    "\n",
    "val model = countVectorizer.fit(phraseDF)\n",
    "\n",
    "model.transform(phraseDF).show(false)\n",
    " \n",
    "println(\"Vocabulaire = \" + model.vocabulary.mkString(\", \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les sélecteurs (Feature Selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorSlicer** permet de sélectionner une liste de colonnes.\n",
    "\n",
    "Il prend en argument une liste d'indices de colonnes et/ou de nom de colonnes, puis produit un nouveau vecteur assembleur avec les colonnes sélectionnées :\n",
    "\n",
    "* Indices des colonnes avec `setIndices`\n",
    "* Nom des colonnes avec `setNames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|features      |SelectedFeatures|\n",
      "+--------------+----------------+\n",
      "|[1.5,3.9,4.2] |[3.9,4.2]       |\n",
      "|[-3.0,1.3,2.5]|[1.3,2.5]       |\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import java.util.Arrays\r\n",
       "import org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}\r\n",
       "import org.apache.spark.ml.feature.VectorSlicer\r\n",
       "import org.apache.spark.ml.linalg.Vectors\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "import org.apache.spark.sql.types.StructType\r\n",
       "data: java.util.List[org.apache.spark.sql.Row] = [[[1.5,3.9,4.2]], [[-3.0,1.3,2.5]]]\r\n",
       "defaultAttr: org.apache.spark.ml.attribute.NumericAttribute = {\"type\":\"numeric\"}\r\n",
       "attrs: Array[org.apache.spark.ml.attribute.NumericAttribute] = Array({\"type\":\"numeric\",\"name\":\"col1\"}, {\"type\":\"numeric\",\"name\":\"col2\"}, {\"type\":\"numeric\",\"name\":\"col3\"})\r\n",
       "attrGroup: org.apache.spark.ml.attribute.AttributeGroup = {\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"col1\"},{\"idx\":1,\"name\":\"col2\"},{\"idx\":2,\"name\":\"col3\"}]},\"num_attrs\":3...\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.util.Arrays\n",
    "\n",
    "import org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}\n",
    "import org.apache.spark.ml.feature.VectorSlicer\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.sql.{Row}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "\n",
    "val data = Arrays.asList(\n",
    "                        Row(Vectors.dense(1.5, 3.9, 4.2)),\n",
    "                        Row(Vectors.dense(-3.0, 1.3, 2.5))\n",
    "                        )\n",
    "\n",
    "val defaultAttr = NumericAttribute.defaultAttr\n",
    "val attrs = Array(\"col1\", \"col2\", \"col3\").map(defaultAttr.withName)\n",
    "\n",
    "val attrGroup = new AttributeGroup(\"features\", attrs.asInstanceOf[Array[Attribute]])\n",
    "\n",
    "val dataset = spark.createDataFrame(data, StructType(Array(attrGroup.toStructField())))\n",
    "\n",
    "val slicer = new VectorSlicer().setInputCol(\"features\").setOutputCol(\"SelectedFeatures\")\n",
    "\n",
    "slicer.setIndices(Array(1)).setNames(Array(\"col3\"))\n",
    "// or slicer.setIndices(Array(1, 2)), or slicer.setNames(Array(\"col1\", \"col2\"))\n",
    "\n",
    "val output = slicer.transform(dataset)\n",
    "output.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ChiSqSelector :** sélectionne des variables catégorielles pour prédire une variable catégorielle en se basant sur le test d'indépendance du chi2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 2 features selected\n",
      "+---+------------------+-----+----------------+\n",
      "| id|          features|label|selectedFeatures|\n",
      "+---+------------------+-----+----------------+\n",
      "|  0|[0.0,5.0,12.0,1.0]|  1.0|      [5.0,12.0]|\n",
      "|  1|[5.0,1.0,1.0,30.0]|  0.0|       [1.0,1.0]|\n",
      "|  2|[1.0,6.0,15.0,1.0]|  0.0|      [6.0,15.0]|\n",
      "|  3|[8.0,3.0,10.0,7.0]|  1.0|      [3.0,10.0]|\n",
      "|  4|[3.0,1.0,2.0,11.0]|  0.0|       [1.0,2.0]|\n",
      "|  5|[1.0,34.0,5.0,1.0]|  1.0|      [34.0,5.0]|\n",
      "+---+------------------+-----+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.ChiSqSelector\r\n",
       "import org.apache.spark.ml.linalg.Vectors\r\n",
       "data: Seq[(Int, org.apache.spark.ml.linalg.Vector, Double)] = List((0,[0.0,5.0,12.0,1.0],1.0), (1,[5.0,1.0,1.0,30.0],0.0), (2,[1.0,6.0,15.0,1.0],0.0), (3,[8.0,3.0,10.0,7.0],1.0), (4,[3.0,1.0,2.0,11.0],0.0), (5,[1.0,34.0,5.0,1.0],1.0))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 1 more field]\r\n",
       "selector: org.apache.spark.ml.feature.ChiSqSelector = chiSqSelector_b767b642980c\r\n",
       "result: org.apache.spark.sql.DataFrame = [id: int, features: vector ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.ChiSqSelector\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "val data = Seq(\n",
    "  (0, Vectors.dense(0.0, 5.0, 12.0, 1.0), 1.0),\n",
    "  (1, Vectors.dense(5.0, 1.0, 1.0, 30.0), 0.0),\n",
    "  (2, Vectors.dense(1.0, 6.0, 15.0, 1.0), 0.0),\n",
    "  (3, Vectors.dense(8.0, 3.0, 10.0, 7.0), 1.0),\n",
    "  (4, Vectors.dense(3.0, 1.0, 2.0, 11.0), 0.0),\n",
    "  (5, Vectors.dense(1.0, 34.0, 5.0, 1.0), 1.0)\n",
    ")\n",
    "\n",
    "val dfm = spark.createDataFrame(data).toDF(\"id\", \"features\", \"label\")\n",
    "\n",
    "val selector = new ChiSqSelector()\n",
    "  .setNumTopFeatures(2)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"label\")\n",
    "  .setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "val result = selector.fit(dfm).transform(dfm)\n",
    "\n",
    "println(s\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\")\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
