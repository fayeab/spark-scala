{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package ml-classification (org.apache.spark.ml.classification)\n",
    "L'objectif est de faire une présentation de la librairie de classification de Spark ML (Scala).\n",
    "\n",
    "**Note :** Je mets plutôt l'accent sur la démarche et non sur la recherche d'un meilleur modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\r\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\r\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\r\n",
       "dfmIris: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 3 more fields]\r\n",
       "dfmIrisLabeled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 4 more fields]\r\n",
       "features: Array[String] = Array(sepal_l, sepal_w, petal_l, petal_w)\r\n",
       "label: String = label\r\n",
       "predCol: String = labelPredictCol\r\n",
       "featureName: String = features\r\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = VectorAssembler: uid=vecAssembler_8adb92dee6d7, handleInvalid=error, numInputCols=4\r\n",
       "dfmIrisAssembled: org.apache.spark.sql.DataFrame = [sepal_l: double, sepal_w: double ... 5 more ...\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "// Charger les données\n",
    "val dfmIris = spark.read.option(\"header\",  true)\n",
    "                   .option(\"inferSchema\",  true)\n",
    "                   .option(\"delimiter\", \",\").csv(\"../data/iris.txt\")\n",
    "\n",
    "// Labeliser les données\n",
    "val dfmIrisLabeled = dfmIris.withColumn(\"label\",  when(col(\"classe\") === \"Iris-virginica\" , 1.0).otherwise(0.0))\n",
    "\n",
    "val features = Array(\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\")\n",
    "val label = \"label\"\n",
    "val predCol = \"labelPredictCol\"\n",
    "val featureName = \"features\"\n",
    "\n",
    "// Assembler les features\n",
    "val assembler = new VectorAssembler().setInputCols(features).setOutputCol(featureName)\n",
    "val dfmIrisAssembled = assembler.transform(dfmIrisLabeled)\n",
    "\n",
    "// Séparer les données en train et test\n",
    "val Array(training, test) = dfmIrisAssembled.randomSplit(Array(0.8, 0.2), seed = 34512)\n",
    "\n",
    "// Evaluer la performance des modèles avec le F1-score\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "                  .setLabelCol(label)\n",
    "                  .setPredictionCol(predCol)\n",
    "                  .setMetricName(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tests d'algorithme de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.9681165936083325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\r\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_e651db27e6cc\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tlogreg_e651db27e6cc-elasticNetParam: 0.5,\r\n",
       "\tlogreg_e651db27e6cc-maxIter: 50,\r\n",
       "\tlogreg_e651db27e6cc-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlogreg_e651db27e6cc-elasticNetParam: 0.5,\r\n",
       "\tlogreg_e651db27e6cc-maxIter: 50,\r\n",
       "\tlogreg_e651db27e6cc-regParam: 0.3\r\n",
       "}, {\r\n",
       "\tlogreg_e651db27e6cc-elasticNetParam: 0.5,\r\n",
       "\tlogreg_e651db27e6cc-maxIter: 50,\r\n",
       "\tlogreg_e651db27e6cc-regParam: 0.7\r\n",
       "}, {\r\n",
       "\tlogreg_e651db27e6cc-elasticNetParam: 0.8,\r\n",
       "\tlogreg_e651db27e6cc-maxIter: 50,\r\n",
       "\tlogreg_e651db27e6cc-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlogreg_e651db27e6cc-elasticNetParam: 0.8,\r\n",
       "\tlogreg_e651db27e6cc-maxIter: 50,\r\n",
       "\tlogreg_e651db27e6cc-regPa...\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "\n",
    "// Regression Logistique\n",
    "val lr = new LogisticRegression()\n",
    "            .setLabelCol(label)\n",
    "            .setFeaturesCol(featureName)\n",
    "            .setPredictionCol(predCol)\n",
    "\n",
    "// Grille de recherche pour trouver le meilleur modèle\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lr.maxIter, Array(50, 100))\n",
    "  .addGrid(lr.regParam,  Array(0.1, 0.3, 0.7))\n",
    "  .addGrid(lr.elasticNetParam,  Array(0.5, 0.8))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(lr)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.8)\n",
    "  .setParallelism(2)\n",
    "\n",
    "// Fitting du modèle\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "// Prediction en test\n",
    "val predictions = model.transform(test)\n",
    "\n",
    "// Calcul du F1-Score sur le test\n",
    "val f1Score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(s\"F1-Score = ${f1Score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Support Vecteurs Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.9368035190615837\n",
      "+-----+---------------+\n",
      "|label|labelPredictCol|\n",
      "+-----+---------------+\n",
      "|  0.0|            0.0|\n",
      "|  0.0|            0.0|\n",
      "|  0.0|            0.0|\n",
      "|  0.0|            0.0|\n",
      "|  0.0|            0.0|\n",
      "|  0.0|            0.0|\n",
      "+-----+---------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LinearSVC\r\n",
       "lsvc: org.apache.spark.ml.classification.LinearSVC = linearsvc_77417142e215\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tlinearsvc_77417142e215-maxIter: 50,\r\n",
       "\tlinearsvc_77417142e215-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlinearsvc_77417142e215-maxIter: 100,\r\n",
       "\tlinearsvc_77417142e215-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tlinearsvc_77417142e215-maxIter: 50,\r\n",
       "\tlinearsvc_77417142e215-regParam: 0.3\r\n",
       "}, {\r\n",
       "\tlinearsvc_77417142e215-maxIter: 100,\r\n",
       "\tlinearsvc_77417142e215-regParam: 0.3\r\n",
       "}, {\r\n",
       "\tlinearsvc_77417142e215-maxIter: 50,\r\n",
       "\tlinearsvc_77417142e215-regParam: 0.7\r\n",
       "}, {\r\n",
       "\tlinearsvc_77417142e215-maxIter: 100,\r\n",
       "\tlinearsvc_77417142e215-regParam: 0.7\r\n",
       "})\r\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_e27581fa7266\r\n",
       "model: org.ap...\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LinearSVC\n",
    "\n",
    "// SVM\n",
    "val lsvc = new LinearSVC()\n",
    "            .setLabelCol(label)\n",
    "            .setFeaturesCol(featureName)\n",
    "            .setPredictionCol(predCol)\n",
    "\n",
    "\n",
    "// Grille de recherche pour trouver le meilleur modèle\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(lsvc.maxIter, Array(50, 100))\n",
    "  .addGrid(lsvc.regParam,  Array(0.1, 0.3, 0.7))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(lsvc)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.8)\n",
    "  .setParallelism(2)\n",
    "\n",
    "// Fitting du modèle\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "// Prediction en test\n",
    "val predictions = model.transform(test)\n",
    "\n",
    "// Calcul du F1-Score sur le test\n",
    "val f1Score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(s\"F1-Score = ${f1Score}\\n\")\n",
    "\n",
    "predictions.select(label, predCol).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Arbre de décision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.9681165936083325\n",
      "Regles de prediction :\n",
      " DecisionTreeClassificationModel: uid=dtc_853c5be9b498, depth=2, numNodes=5, numClasses=2, numFeatures=4\n",
      "  If (feature 2 <= 4.75)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 4.75)\n",
      "   If (feature 3 <= 1.75)\n",
      "    Predict: 0.0\n",
      "   Else (feature 3 > 1.75)\n",
      "    Predict: 1.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel}\r\n",
       "tree: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_853c5be9b498\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tdtc_853c5be9b498-impurity: entropy,\r\n",
       "\tdtc_853c5be9b498-maxDepth: 7,\r\n",
       "\tdtc_853c5be9b498-minInstancesPerNode: 3\r\n",
       "}, {\r\n",
       "\tdtc_853c5be9b498-impurity: gini,\r\n",
       "\tdtc_853c5be9b498-maxDepth: 7,\r\n",
       "\tdtc_853c5be9b498-minInstancesPerNode: 3\r\n",
       "}, {\r\n",
       "\tdtc_853c5be9b498-impurity: entropy,\r\n",
       "\tdtc_853c5be9b498-maxDepth: 10,\r\n",
       "\tdtc_853c5be9b498-minInstancesPerNode: 3\r\n",
       "}, {\r\n",
       "\tdtc_853c5be9b498-impurity: gini,\r\n",
       "\tdtc_853c5be9b498-maxDepth: 10,\r\n",
       "\tdtc_853c5be9b498-minInstancesPerNode: 3\r\n",
       "}, {\r\n",
       "\tdtc_853c5be9b498-impurity: entropy,\r\n",
       "\tdtc_853c5be9b498-maxDepth: 7,\r\n",
       "\tdtc_853c...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier, DecisionTreeClassificationModel}\n",
    "\n",
    "// Arbre de décision\n",
    "val tree = new DecisionTreeClassifier()\n",
    "            .setLabelCol(label)\n",
    "            .setFeaturesCol(featureName)\n",
    "            .setPredictionCol(predCol)\n",
    "\n",
    "// Grille de recherche pour trouver le meilleur modèle\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(tree.maxDepth, Array(7, 10))\n",
    "  .addGrid(tree.impurity, Array(\"entropy\", \"gini\"))\n",
    "  .addGrid(tree.minInstancesPerNode, Array(3, 5))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(tree)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.8)\n",
    "  .setParallelism(2)\n",
    "\n",
    "// Fitting du modèle\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "// Prediction en test\n",
    "val predictions = model.transform(test)\n",
    "\n",
    "// Calcul du F1-Score sur le test\n",
    "val f1Score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(s\"F1-Score = ${f1Score}\\n\")\n",
    "\n",
    "// Afficher l'arbre de décision\n",
    "val treeModel = model.bestModel.asInstanceOf[DecisionTreeClassificationModel]\n",
    "println(s\"Regles de prediction :\\n ${treeModel.toDebugString}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Forêts aléatoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score = 0.9681165936083325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\r\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_dc0510037180\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\trfc_dc0510037180-featureSubsetStrategy: sqrt,\r\n",
       "\trfc_dc0510037180-impurity: gini,\r\n",
       "\trfc_dc0510037180-maxDepth: 5,\r\n",
       "\trfc_dc0510037180-minInstancesPerNode: 3,\r\n",
       "\trfc_dc0510037180-numTrees: 10,\r\n",
       "\trfc_dc0510037180-subsamplingRate: 0.6\r\n",
       "}, {\r\n",
       "\trfc_dc0510037180-featureSubsetStrategy: sqrt,\r\n",
       "\trfc_dc0510037180-impurity: gini,\r\n",
       "\trfc_dc0510037180-maxDepth: 5,\r\n",
       "\trfc_dc0510037180-minInstancesPerNode: 3,\r\n",
       "\trfc_dc0510037180-numTrees: 10,\r\n",
       "\trfc_dc0510037180-subsamplingRate: 0.8\r\n",
       "}, {\r\n",
       "\trfc_dc0510037180-featureSubsetStrategy: sqrt,\r\n",
       "\trfc_dc0510037180-impurity: entropy,\r\n",
       "\trfc_dc0510037180-maxDep...\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "\n",
    "// Random Forest\n",
    "val rf = new RandomForestClassifier()\n",
    "            .setLabelCol(label)\n",
    "            .setFeaturesCol(featureName)\n",
    "            .setPredictionCol(predCol)\n",
    "\n",
    "// Grille de recherche pour trouver le meilleur modèle\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(rf.maxDepth, Array(5, 8, 12))\n",
    "  .addGrid(rf.impurity,  Array(\"gini\", \"entropy\"))\n",
    "  .addGrid(rf.subsamplingRate, Array(0.6, 0.8))\n",
    "  .addGrid(rf.minInstancesPerNode, Array(3, 5))\n",
    "  .addGrid(rf.numTrees, Array(10, 20, 40))\n",
    "  .addGrid(rf.featureSubsetStrategy, Array(\"sqrt\", \"log2\"))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(rf)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.8)\n",
    "  .setParallelism(2)\n",
    "\n",
    "// Fitting du modèle\n",
    "val model = trainValidationSplit.fit(training)\n",
    "\n",
    "// Prediction en test\n",
    "val predictions = model.transform(test)\n",
    "\n",
    "// Calcul du F1-Score sur le test\n",
    "val f1Score = evaluator.evaluate(predictions)\n",
    "\n",
    "print(s\"F1-Score = ${f1Score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Références :**  \n",
    "[Documentation Spark](https://spark.apache.org/docs/3.0.0/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
