{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark context ##\n",
    "\n",
    "Le `spark context` est le principal point d'accès aux fonctionnalités de Spark. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration de Spark \n",
    "\n",
    "Quelques éléments pour configurer Spark. Pour plus d'information, vous visitez le [guide de l'utilisateur Amazon EMR](https://docs.aws.amazon.com/emr/index.html)\n",
    "\n",
    "* **spark.master** : URL du master\n",
    "* **spark.driver.memory** : Volume de mémoire à utiliser pour le processus de pilote, c'est-à-dire où SparkContext est initialisé.\n",
    "* **spark.driver.cores** : Nombre de cœurs à utiliser sur le driver.\n",
    "* **spark.executor.memory** : Volume de mémoire à utiliser par les processus de l'exécuteur.\n",
    "* **spark.executor.instances** : Le nombre d'exécuteurs\n",
    "* **spark.executor.cores** : Nombre de cœurs à utiliser sur chaque exécuteur.\n",
    "* **spark.default.parallelism** : Nombre par défaut de partitions dans DDR renvoyées par des transformations telles que join, reduceByKey et parallelize lorsque l'utilisateur ne s'occupe pas de la définition.\n",
    "* **spark.dynamicAllocation.enabled** :  Décision d'utiliser ou non une affectation des ressources dynamique, qui adapte à la hausse ou à la baisse le nombre d'exécuteurs inscrits auprès d'une application en fonction de la charge de travail.\n",
    "\n",
    "* **spark.dynamicAllocation.minExecutors** : Limite inférieure pour le nombre d'exécuteurs si l'attribution dynamique est activée.\n",
    "* **spark.dynamicAllocation.maxExecutors** : Limite supérieure pour le nombre d'exécuteurs si l'attribution dynamique est activée.\n",
    "* **spark.submit.deployMode**: Type d’exécution d'une application\n",
    "   * Mode client : le driver est créé dans la machine qui soumet l’application\n",
    "   * Mode cluster : le driver est créé à l’intérieur du cluster (dans une machine worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un Spark context\n",
    "\n",
    "* On ne peut pas créer plus d'un Spark context par session. S'il est dèja créé, il faut l'arrêter avec cette commande `sc.stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.{SparkConf, SparkContext}\r\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@517372d0\r\n",
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@6236069c\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "val conf = new SparkConf()\n",
    "   conf.set(\"spark.master\", \"local[2]\")\n",
    "   conf.set(\"spark.app.name\", \"AppName\")\n",
    "   conf.set(\"spark.broadcast.compress\", \"false\")\n",
    "   conf.set(\"spark.shuffle.compress\", \"false\")\n",
    "   conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "\n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.id => local-1610199821954\n",
      "spark.driver.host => STAR-L046\n",
      "spark.app.name => AppName\n",
      "spark.executor.id => driver\n",
      "spark.repl.class.outputDir => C:\\Users\\fayab\\AppData\\Local\\Temp\\tmpjnfx5hn0\n",
      "spark.serializer => org.apache.spark.serializer.KryoSerializer\n",
      "spark.broadcast.compress => false\n",
      "spark.repl.class.uri => spark://STAR-L046:52499/classes\n",
      "spark.submit.pyFiles => \n",
      "spark.submit.deployMode => client\n",
      "spark.driver.port => 52499\n",
      "spark.ui.showConsoleProgress => true\n",
      "spark.master => local[2]\n",
      "spark.shuffle.compress => false\n"
     ]
    }
   ],
   "source": [
    "sc.getConf.getAll.foreach(x => println(s\"${x._1} => ${x._2}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une Spark Session\n",
    "\n",
    "Au début du projet Spark, chaque API dispose de son contexte (sqlContext, HiveContext, ..). Mais depuis la version Apache Spark 2.0, SparkSession permet accéder à toutes les fonctionnalités de Spark. Il regroupe differents contextes :\n",
    "* Spark Context\n",
    "* SQL Context\n",
    "* Streaming Context\n",
    "* Hive Context\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2255485e\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "      .enableHiveSupport()\n",
    "      .config(conf)\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une Spark Session contient un spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.id => local-1610199821954\n",
      "spark.driver.host => STAR-L046\n",
      "spark.app.name => AppName\n",
      "spark.executor.id => driver\n",
      "spark.repl.class.outputDir => C:\\Users\\fayab\\AppData\\Local\\Temp\\tmpjnfx5hn0\n",
      "spark.serializer => org.apache.spark.serializer.KryoSerializer\n",
      "spark.broadcast.compress => false\n",
      "spark.repl.class.uri => spark://STAR-L046:52499/classes\n",
      "spark.submit.pyFiles => \n",
      "spark.submit.deployMode => client\n",
      "spark.driver.port => 52499\n",
      "spark.ui.showConsoleProgress => true\n",
      "spark.master => local[2]\n",
      "spark.shuffle.compress => false\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.getConf.getAll.foreach(x => println(s\"${x._1} => ${x._2}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un SQL Context\n",
    "\n",
    "* SQLContext est le point d'accès à SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SQLContext\r\n",
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@a59e694\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une Spark Session contient un SQL context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@7cc62cc0\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un Hive Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.hive.HiveContext\r\n",
       "hiveContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@6e3319e9\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.hive.HiveContext\n",
    "\n",
    "val hiveContext = new HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une Spark Session peut accéder directement à Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**  \n",
    "[Documentation Spark](https://spark.apache.org/docs/3.0.0/)     \n",
    "[Amazon EMR: Guide de l'utilisateur Amazon EMR](https://docs.aws.amazon.com/emr/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
