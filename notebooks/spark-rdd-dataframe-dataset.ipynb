{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis la version 2.0 d'Apache Spark, la SparkSession a été créée afin d'unifier l'accès aux RDD, Dataframes et Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RDD (Resilient Distributed Datasets)\n",
    "\n",
    "Un RDD est une collection distribuée d’enregistrements: Un RDD possède plusieurs caratérisques dont:\n",
    "\n",
    "* Immuable: Une fois créée, un RDD reste inchangé. Pour obtenir une modification d’un RDD, il faut y appliquer une transformation, qui retournera un nouveau RDD,\n",
    "* tolérance aux pannes : un RDD sait comment recréer et recalculer son ensemble de données grace au DAG (Directed Acyclic Graph).\n",
    "\n",
    "Les RDD supportent deux types d’opérations :\n",
    "* Transformation : c'est une opération qui retourne un nouveau RDD\n",
    "    * exemple: `map`, `filter`, `flatMap`, `groupByKey`, `reduceByKey`,`aggregateByKey`.\n",
    "\n",
    "* Action: Une action évalue et retourne une nouvelle valeur\n",
    "    * exemple: `count`, `take`, `first`, `countByKey`, `collect`,`...`.\n",
    "    \n",
    "Les transformations sont paresseuses (lazy evaluations) car elles sont calculées seulement que lorsqu'une action est appliquée au RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@539e0445\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"AppName\") \n",
    "                        .enableHiveSupport().getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un RDD à partir d’une collection scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java\n",
      "Python\n",
      "Ruby\n",
      "R\n",
      "JavaScript\n",
      "Scala\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "langages: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:27\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val langages = spark.sparkContext.parallelize(Seq(\"Java\", \"Scala\", \"R\", \"Python\", \"Ruby\", \"JavaScript\"))\n",
    "langages.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un RDD à partir d'une source de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.rdd.RDD[String] = ../data/frostroad.txt MapPartitionsRDD[2] at textFile at <console>:27\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = spark.sparkContext.textFile(\"../data/frostroad.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quelques transformations et actions sur les RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[String] = Array(Two roads diverged in a yellow wood,, And sorry I could not travel both, And be one traveler, long I stood, And looked down one as far as I could, To where it bent in the undergrowth;, \"\", Then took the other, as just as fair,, And having perhaps the better claim,, Because it was grassy and wanted wear;, Though as for that the passing there, Had worn them really about the same,, \"\", And both that morning equally lay, In leaves no step had trodden black., Oh, I kept the first for another day!, Yet knowing how way leads on to way,, I doubted if I should ever come back., \"\", I shall be telling this with a sigh, Somewhere ages and ages hence:, Two roads diverged in a wood, and I--, I took the one less traveled by,, And that has made all the difference.)\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Long = 23\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n"
     ]
    }
   ],
   "source": [
    "for (sent <- data.take(2)) \n",
    "    println(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWO ROADS DIVERGED IN A YELLOW WOOD,\n",
      "AND SORRY I COULD NOT TRAVEL BOTH\n",
      "AND BE ONE TRAVELER, LONG I STOOD\n"
     ]
    }
   ],
   "source": [
    "data.map(line => line.toUpperCase).take(3).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In leaves no step had trodden black.\n",
      "I doubted if I should ever come back.\n",
      "I shall be telling this with a sigh\n",
      "I took the one less traveled by,\n"
     ]
    }
   ],
   "source": [
    "data.filter(line => line.startsWith(\"I\")).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Création d'un RDD à partir d'un autre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataFiltre: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[6] at filter at <console>:27\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataFiltre = data.map(line => line.toUpperCase).filter(line => line.startsWith(\"I\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN LEAVES NO STEP HAD TRODDEN BLACK.\n",
      "I DOUBTED IF I SHOULD EVER COME BACK.\n",
      "I SHALL BE TELLING THIS WITH A SIGH\n",
      "I TOOK THE ONE LESS TRAVELED BY,\n"
     ]
    }
   ],
   "source": [
    "dataFiltre.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "/** La  fonction toDebugString() qui fournit un String décrivant\n",
    "    le DAG de transformations permettant d’obtenir ce RDD */"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: String =\r\n",
       "(2) MapPartitionsRDD[6] at filter at <console>:27 []\r\n",
       " |  MapPartitionsRDD[5] at map at <console>:27 []\r\n",
       " |  ../data/frostroad.txt MapPartitionsRDD[2] at textFile at <console>:27 []\r\n",
       " |  ../data/frostroad.txt HadoopRDD[1] at textFile at <console>:27 []\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFiltre.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DataFrame et DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis la version 2.0 de Spark, les deux API (DataFrame, DataSet ) sont unifiées en une seule.\n",
    "* Sacla : un dataframe est un alias de Dataset\n",
    "* En Java, on retrouve principalement des Dataset\n",
    "* R et Python utilisement les dataframes car ces 2 langages ne sont pas compilés.\n",
    "\n",
    "<img src=\"../images/databricks.png\" style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame\n",
    "\n",
    "Un DataFrame est une collection distribuée de données organisées en colonne:\n",
    "* posséde également les caratistiques d'un RDD (immuabilité, tolérance aux pannes, les types d'opérations, etc, ...). \n",
    "* dispose aussi d'un catalyseur pour optimiser les traitements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un RDD à partir d’une collection scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "simpleData: Seq[(String, String, Int)] = List((James,Sales,3000), (Michael,Sales,4600), (Robert,Sales,4100), (Maria,Finance,3000), (James,Sales,3000), (Scott,Finance,3300), (Jen,Finance,3900), (Jeff,Marketing,3000), (Kumar,Marketing,2000), (Saif,Sales,4100))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [employee_name: string, department: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val simpleData = Seq((\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  )\n",
    "\n",
    "val dfm = simpleData.toDF(\"employee_name\", \"department\", \"salary\")\n",
    "dfm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quelques transformations et actions sur les dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|JAMES        |Sales     |3000  |\n",
      "|MICHAEL      |Sales     |4600  |\n",
      "|ROBERT       |Sales     |4100  |\n",
      "|MARIA        |Finance   |3000  |\n",
      "|JAMES        |Sales     |3000  |\n",
      "|SCOTT        |Finance   |3300  |\n",
      "|JEN          |Finance   |3900  |\n",
      "|JEFF         |Marketing |3000  |\n",
      "|KUMAR        |Marketing |2000  |\n",
      "|SAIF         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.upper\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.upper\n",
    "dfm.withColumn(\"employee_name\", upper($\"employee_name\")).show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|James        |Sales     |3000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfm.filter(dfm(\"department\") === \"Sales\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           salary|\n",
      "+-------+-----------------+\n",
      "|  count|               10|\n",
      "|   mean|           3400.0|\n",
      "| stddev|765.9416862050705|\n",
      "|    min|             2000|\n",
      "|    max|             4600|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfm.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Création d'un RDD à partir d'un autre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|JAMES        |Sales     |3000  |\n",
      "|MICHAEL      |Sales     |4600  |\n",
      "|ROBERT       |Sales     |4100  |\n",
      "|JAMES        |Sales     |3000  |\n",
      "|SAIF         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dfm2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [employee_name: string, department: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfm2 = dfm.withColumn(\"employee_name\", upper($\"employee_name\"))\n",
    "              .filter(dfm(\"department\") === \"Sales\")\n",
    "\n",
    "dfm2.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Filter (department#11 = Sales)\n",
      "+- Project [upper(employee_name#10) AS employee_name#130, department#11, salary#12]\n",
      "   +- Project [_1#3 AS employee_name#10, _2#4 AS department#11, _3#5 AS salary#12]\n",
      "      +- LocalRelation [_1#3, _2#4, _3#5]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "employee_name: string, department: string, salary: int\n",
      "Filter (department#11 = Sales)\n",
      "+- Project [upper(employee_name#10) AS employee_name#130, department#11, salary#12]\n",
      "   +- Project [_1#3 AS employee_name#10, _2#4 AS department#11, _3#5 AS salary#12]\n",
      "      +- LocalRelation [_1#3, _2#4, _3#5]\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LocalRelation [employee_name#130, department#11, salary#12]\n",
      "\n",
      "== Physical Plan ==\n",
      "LocalTableScan [employee_name#130, department#11, salary#12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfm2.explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un RDD à partir d'une source de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+--------------------+\n",
      "|brand|      name|    device|               model|\n",
      "+-----+----------+----------+--------------------+\n",
      "|     |          |    AD681H|Smartfren Androma...|\n",
      "|     |          |     FJL21|               FJL21|\n",
      "|     |          |  hws7721g|  MediaPad 7 Youth 2|\n",
      "|  1&1|  1&1 Puck|diw362_1u1|         DIW362P 1U1|\n",
      "|  1&1|1&1 TV Box|diw387_1u1|          DIW387 1U1|\n",
      "+-----+----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "import org.apache.spark.sql.types.{StructType, StringType}\r\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(brand,StringType,true), StructField(name,StringType,true), StructField(device,StringType,true), StructField(model,StringType,true))\r\n",
       "dfm: org.apache.spark.sql.DataFrame = [brand: string, name: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Define custom schema\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.types.{StructType, StringType}\n",
    "val schema = new StructType()\n",
    "      .add(\"brand\", StringType, true)\n",
    "      .add(\"name\", StringType, true)\n",
    "      .add(\"device\", StringType, true)\n",
    "      .add(\"model\", StringType, true)\n",
    "val dfm = spark.read.schema(schema).option(\"multiline\", \"true\").json(\"../data/devices.json\")\n",
    "dfm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Un dataset est une extension d'un dataframe:\n",
    "* fortement typé, immuable collection d'objets qui sont associés à un schéma relationnel.\n",
    "* dispose d'un encodeur pour gérer et valider les types des objets\n",
    "* détecte les erreurs de syntaxe et d'analyse au moment de la compilation du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un RDD à partir d’une collection scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|  Java|\n",
      "|Python|\n",
      "|     R|\n",
      "| Scala|\n",
      "|  Ruby|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset: org.apache.spark.sql.Dataset[String] = [value: string]\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataset = Seq(\"Java\", \"Python\", \"R\", \"Scala\", \"Ruby\").toDS()\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un dataset à partir d'un RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----+\n",
      "|     _1|       _2|  _3|\n",
      "+-------+---------+----+\n",
      "|  James|    Sales|3000|\n",
      "|Michael|    Sales|4600|\n",
      "| Robert|    Sales|4100|\n",
      "|  Maria|  Finance|3000|\n",
      "|  James|    Sales|3000|\n",
      "|  Scott|  Finance|3300|\n",
      "|    Jen|  Finance|3900|\n",
      "|   Jeff|Marketing|3000|\n",
      "|  Kumar|Marketing|2000|\n",
      "|   Saif|    Sales|4100|\n",
      "+-------+---------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "simpleData: Seq[(String, String, Int)] = List((James,Sales,3000), (Michael,Sales,4600), (Robert,Sales,4100), (Maria,Finance,3000), (James,Sales,3000), (Scott,Finance,3300), (Jen,Finance,3900), (Jeff,Marketing,3000), (Kumar,Marketing,2000), (Saif,Sales,4100))\r\n",
       "rdd: org.apache.spark.rdd.RDD[(String, String, Int)] = ParallelCollectionRDD[16] at parallelize at <console>:50\r\n",
       "ds: org.apache.spark.sql.Dataset[(String, String, Int)] = [_1: string, _2: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simpleData = Seq((\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  )\n",
    "\n",
    "val rdd = sc.parallelize(simpleData)\n",
    "val ds = rdd.toDS()\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation d'un dataset à partir d'un dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|Langage|Note|\n",
      "+-------+----+\n",
      "|   Java|  10|\n",
      "|      R|   5|\n",
      "|  Scala|  23|\n",
      "| Python|  15|\n",
      "+-------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[20] at parallelize at <console>:41\r\n",
       "df: org.apache.spark.sql.DataFrame = [Langage: string, Note: int]\r\n",
       "ds: org.apache.spark.sql.Dataset[(String, Int)] = [Langage: string, Note: int]\r\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Sans Case Classe\n",
    "\n",
    "val rdd = sc.parallelize(Seq((\"Java\", 10), (\"R\", 5), (\"Scala\", 23), (\"Python\", 15)))\n",
    "val df = rdd.toDF(\"Langage\", \"Note\")\n",
    "\n",
    "val ds = df.as[(String, Int)]\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "defined class Employee\r\n",
       "simpleData: Seq[Employee] = List(Employee(James,Sales,3000), Employee(Michael,Sales,4600), Employee(Robert,Sales,4100), Employee(Maria,Finance,3000), Employee(James,Sales,3000), Employee(Scott,Finance,3300), Employee(Jen,Finance,3900), Employee(Jeff,Marketing,3000), Employee(Kumar,Marketing,2000), Employee(Saif,Sales,4100))\r\n",
       "df: org.apache.spark.sql.DataFrame = [employee_name: string, department: string ... 1 more field]\r\n",
       "empDS: org.apache.spark.sql.Dataset[Employee] = [employee_name: string, department: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Avec Case class\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "case class Employee (employee_name: String, department: String, salary: Long)\n",
    "\n",
    "val simpleData = Seq(Employee(\"James\", \"Sales\", 3000),\n",
    "    Employee(\"Michael\", \"Sales\", 4600),\n",
    "    Employee(\"Robert\", \"Sales\", 4100),\n",
    "    Employee(\"Maria\", \"Finance\", 3000),\n",
    "    Employee(\"James\", \"Sales\", 3000),\n",
    "    Employee(\"Scott\", \"Finance\", 3300),\n",
    "    Employee(\"Jen\", \"Finance\", 3900),\n",
    "    Employee(\"Jeff\", \"Marketing\", 3000),\n",
    "    Employee(\"Kumar\", \"Marketing\", 2000),\n",
    "    Employee(\"Saif\", \"Sales\", 4100)\n",
    "  )\n",
    "\n",
    "val df = simpleData.toDF()\n",
    "\n",
    "val empDS = df.as[Employee]\n",
    "empDS.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "[Documentation Databricks](https://databricks.com/fr/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
