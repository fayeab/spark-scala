{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center> Quelques techniques pour optimiser une application Spark </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Broadcasting\n",
    "\n",
    "Lorsque vous exécutez une application Spark, les jobs sont envoyés aux différents workers entrainant beaucoup d'échanges de données ces derniers et le driver. Par exemple :\n",
    "\n",
    "```scala\n",
    "val  rdd = sc.parallelize(Seq(1, 2, 3, 4))\n",
    "val  const = 10\n",
    "rdd.map(x => x + const).collect\n",
    "```\n",
    "\n",
    "Comme les données du RDD sont partitionnées, chaque exécuteur reçoit une copie de la variable `const` car celle-ci est définie sur le driver. Cela peut provoquer des problèmes de performances notamment lors d'une jointure. La technique d'un broadcasting qui permet d'optimiser ce type de problème en rendant disponible une variable (par exemple `const ` dans l'exemple) à tous les nœuds. \n",
    "Remarques : \n",
    "\n",
    "* Le broadcasting est contrôlée par le paramètre de configuration `spark.sql.autoBroadcastJoinThreshold`, dont la valeur par défaut est de 10 Mo.\n",
    "* Une variable broadcastée n'est pas modifiable (read-only variable). Cela permet de s'assurer que tous les workers utilisent la même valeur.\n",
    "* Pour les fonctions parallélisées utilisant en argument une donnée relativement importante, l'utilisation du broadcasting peut améliorer la performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+\n",
      "|EMP_NAME|DEP_CD|SALARY|\n",
      "+--------+------+------+\n",
      "|   James|   SLS|  3000|\n",
      "| Michael|   SLS|  4600|\n",
      "|  Robert|   SLS|  4100|\n",
      "|   Maria|   FIN|  3000|\n",
      "|   James|   SLS|  3000|\n",
      "|   Scott|   FIN|  3300|\n",
      "|     Jen|   FIN|  3900|\n",
      "|    Jeff|   MKT|  3000|\n",
      "|   Kumar|   MKT|  2000|\n",
      "|    Saif|   SLS|  4100|\n",
      "+--------+------+------+\n",
      "\n",
      "+--------+---------+------+\n",
      "|EMP_NAME|   DEP_CD|SALARY|\n",
      "+--------+---------+------+\n",
      "|   James|    Sales|  3000|\n",
      "| Michael|    Sales|  4600|\n",
      "|  Robert|    Sales|  4100|\n",
      "|   Maria|  Finance|  3000|\n",
      "|   James|    Sales|  3000|\n",
      "|   Scott|  Finance|  3300|\n",
      "|     Jen|  Finance|  3900|\n",
      "|    Jeff|Marketing|  3000|\n",
      "|   Kumar|Marketing|  2000|\n",
      "|    Saif|    Sales|  4100|\n",
      "+--------+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "simpleData: Seq[(String, String, Int)] = List((James,SLS,3000), (Michael,SLS,4600), (Robert,SLS,4100), (Maria,FIN,3000), (James,SLS,3000), (Scott,FIN,3300), (Jen,FIN,3900), (Jeff,MKT,3000), (Kumar,MKT,2000), (Saif,SLS,4100))\r\n",
       "employeDF: org.apache.spark.sql.DataFrame = [EMP_NAME: string, DEP_CD: string ... 1 more field]\r\n",
       "depDF: org.apache.spark.sql.DataFrame = [DEP_CD: string, DEP_LIB: string]\r\n",
       "deptMap: scala.collection.immutable.Map[String,String] = Map(SLS -> Sales, FIN -> Finance, MKT -> Marketing)\r\n",
       "import org.apache.spark.sql.functions.{col, udf}\r\n",
       "import org.apache.spark.broadcast.Broadcast\r\n",
       "mapping: (maps: org.apache.spark.broadcast.Broadcast[Map[String,String]])String => String\r\n",
       "func: String => String = $Lambda$2528/1686643672@1f171d23\r\n",
       "transformer: org.a...\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val simpleData = Seq((\"James\", \"SLS\", 3000),\n",
    "    (\"Michael\", \"SLS\", 4600),\n",
    "    (\"Robert\", \"SLS\", 4100),\n",
    "    (\"Maria\", \"FIN\", 3000),\n",
    "    (\"James\", \"SLS\", 3000),\n",
    "    (\"Scott\", \"FIN\", 3300),\n",
    "    (\"Jen\", \"FIN\", 3900),\n",
    "    (\"Jeff\", \"MKT\", 3000),\n",
    "    (\"Kumar\", \"MKT\", 2000),\n",
    "    (\"Saif\", \"SLS\", 4100)\n",
    "  )\n",
    "\n",
    "val employeDF = simpleData.toDF(\"EMP_NAME\", \"DEP_CD\", \"SALARY\")\n",
    "employeDF.show()\n",
    "\n",
    "val depDF = Seq(\n",
    "  (\"SLS\", \"Sales\"),\n",
    "  (\"FIN\", \"Finance\"),\n",
    "  (\"MKT\", \"Marketing\")\n",
    ").toDF(\"DEP_CD\", \"DEP_LIB\")\n",
    "\n",
    "val deptMap = Map(\"SLS\" -> \"Sales\",\n",
    "                    \"FIN\" -> \"Finance\",\n",
    "                    \"MKT\" -> \"Marketing\")\n",
    "\n",
    "import org.apache.spark.sql.functions.{col, udf}\n",
    "import org.apache.spark.broadcast.Broadcast\n",
    "\n",
    "def mapping(maps: Broadcast[Map[String, String]]) : String => String = {x => maps.value.getOrElse(x, \"No. Dep\")}\n",
    "val func = mapping(sc.broadcast(deptMap))\n",
    "val transformer = udf(func)\n",
    "\n",
    "employeDF.withColumn(\"DEP_CD\", transformer(col(\"DEP_CD\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accumulateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un accumulateur est une variable globale donc modifiable par les workers. Il n'accepte que des opérations associatives ou commutatives.\n",
    "\n",
    "Les workers peuvent modifier la valeur d'un accumulateur mais seul le driver peut lire sa valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 5, name: Some(Exemple accumulateur), value: 10)\r\n",
       "res2: Long = 10\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accum = sc.longAccumulator(\"Exemple accumulateur\")\n",
    "\n",
    "sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum.add(x))\n",
    " \n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quelles fonctions à éviter quand c'est possible\n",
    "\n",
    "* `collect` : envoie toutes les données vers le driver       \n",
    "\n",
    "* `count` : cette fonction prend beaucoup de temps\n",
    "\n",
    "* `groupByKey` Il est préférable d'utiliser `reduceByKey` au lieu de `groupByKey` pour avoir une meilleure performance en termes de temps de calcul et de mémoire utilisé. Pour avoir plus d'options, vous pouvez utiliser `combineByKey` (`reduceByKey` est implémenté en utilisant `combineByKey`)\n",
    "\n",
    "* Les fonctions `udf` peuvent prendre beaucoup car elles ne sont pas souvent bien optimisées. Donc elles sont à éviter quand c'est possible surtout pour les utilisateurs de PySpark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cache et Persistence\n",
    "\n",
    "Lorsqu’on accède à un dataframe de façon itérative, utiliser la technique du caching peut améliorer les performances. Pour mettre en cache des données, Spark fournit deux fonctions :\n",
    "\n",
    "* `cache` : stocke la donnée en mémoire\n",
    "* `persist` : stocke la donnée en mémoire et / ou sur le disque\n",
    "* `cache = persit(StorageLevel.MEMORY_ONLY)`\n",
    "\n",
    "Ce n'est pas toujours pertinent de mettre en cache des données. Lorsqu'on lit une fois des données, il n'est pas souvent nécessaire de mettre en cache car cela ne peut ralentir votre travail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sérialisation des données\n",
    "\n",
    "Le calcul distribué de Spark entraine d'importantes étapes de sérialisation de données. En fonction du format utilisé, ces étapes peuvent très coûteuses en temps. Par défaut, Spark utilise la sérialisation Java. Mais vous pouvez utiliser sérialisation Kryo (parfois 10 fois rapide).\n",
    "\n",
    "`conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Répartition des  Données\n",
    "\n",
    "Le re-partitionnement des données est une piste pour améliorer les performances d'une application Spark.\n",
    "\n",
    "* Eviter les partitions trop petites : cela peut entrainer de beaucoup de petites tâches à effectuer. Vous pouvez répartir à nouveau les données en utilisant :\n",
    "    * `repartition(n)` \n",
    "    * ou `coalesce(n, shuffle=true)`\n",
    "* Eviter les partitions déséquilibrées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**    \n",
    "\n",
    "[Documentation Spark](https://spark.apache.org/docs/latest/sql-performance-tuning.html)     \n",
    "[Documentation Microsoft](https://docs.microsoft.com/fr-fr/azure/hdinsight/spark/apache-spark-overview)   \n",
    "[Apache Spark Optimisation Techniques and Performance Tuning](https://www.xenonstack.com/blog/apache-spark-optimisation/)    \n",
    "[Apache Spark - Best practices and tuning](https://umbertogriffo.gitbook.io/apache-spark-best-practices-and-tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
