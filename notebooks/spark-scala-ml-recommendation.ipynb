{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package ml-recommendation (org.apache.spark.ml.recommendation)\n",
    "\n",
    "L'objectif est de faire une présentation de la librairie de recommendation de Spark ML (Scala).\n",
    "\n",
    "**Note :** Je mets plutôt l'accent sur la démarche et non sur la recherche d'un meilleur modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Factorisation  de matrice\n",
    "\n",
    "La factorisation de matrice est l'une des techniques les plus utilisées pour construire un système de recommandation. Il s'agit de décomposer une matrice en un produit de matrices. En d'autres termes, il faut retrouver des matrices dont le produit est une approximation de la matrice à factoriser. Dans Spark ce problème de minimisation est résolu par l’algorithme itératif Alternating Least Squares (**ALS**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recommandation de livres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+\n",
      "|user_id|book_id|rating|\n",
      "+-------+-------+------+\n",
      "|      1|    258|     5|\n",
      "|      2|   4081|     4|\n",
      "|      2|    260|     5|\n",
      "+-------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\r\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\r\n",
       "import org.apache.spark.ml.recommendation.ALS\r\n",
       "import scala.collection.mutable.WrappedArray\r\n",
       "import org.apache.spark.sql.Row\r\n",
       "ratingsDF: org.apache.spark.sql.DataFrame = [user_id: int, book_id: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "import scala.collection.mutable.WrappedArray\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "// Charger les données\n",
    "val ratingsDF = spark.read.option(\"header\",  true)\n",
    "                   .option(\"inferSchema\",  true)\n",
    "                   .option(\"delimiter\", \",\").csv(\"../data/ratings.txt\")\n",
    "\n",
    "ratingsDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 5976479\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsDF.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, book_id: int ... 1 more field]\r\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int, book_id: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Séparer les données en train et test\n",
    "val Array(training, test) = ratingsDF.randomSplit(Array(0.8, 0.2), seed = 34512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "als: org.apache.spark.ml.recommendation.ALS = als_613446f4fea0\r\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_592a755b91c1, metricName=rmse, throughOrigin=false\r\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\r\n",
       "Array({\r\n",
       "\tals_613446f4fea0-maxIter: 5,\r\n",
       "\tals_613446f4fea0-rank: 5,\r\n",
       "\tals_613446f4fea0-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tals_613446f4fea0-maxIter: 5,\r\n",
       "\tals_613446f4fea0-rank: 15,\r\n",
       "\tals_613446f4fea0-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tals_613446f4fea0-maxIter: 5,\r\n",
       "\tals_613446f4fea0-rank: 25,\r\n",
       "\tals_613446f4fea0-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tals_613446f4fea0-maxIter: 5,\r\n",
       "\tals_613446f4fea0-rank: 100,\r\n",
       "\tals_613446f4fea0-regParam: 0.1\r\n",
       "}, {\r\n",
       "\tals_613446f4fea0-maxIter: 5,\r\n",
       "\tals_613446f4fea0-rank: 5,\r\n",
       "\tals_613446f4fea0-regParam: 0.3\r\n",
       "}, {\r\n",
       "\tals_613446f4fea0-maxIter...\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Algo ALS\n",
    "val als = new ALS()\n",
    "  .setUserCol(\"user_id\")\n",
    "  .setItemCol(\"book_id\")\n",
    "  .setRatingCol(\"rating\")\n",
    "  .setSeed(12345)\n",
    "  .setNonnegative(true)  // Ajouter une contrainte pour avoir des matrices non négatives.\n",
    "  .setImplicitPrefs(false) // Si les infos ont été recuillies de façon implite (achats, clics, etc..). \n",
    "  .setColdStartStrategy(\"drop\") // l'erreur d'ajustement de la factorisation ne prend pas en compte les NA.\n",
    "\n",
    "// Calcul du RMSE sur le test\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setMetricName(\"rmse\")\n",
    "  .setLabelCol(\"rating\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "\n",
    "// Grille de recherche pour trouver le meilleur modèle\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(als.rank, Array(5, 15, 25, 100)) // nombre de facteurs latents\n",
    "  .addGrid(als.regParam,  Array(0.1, 0.3, 0.7)) // paramètre de régularisation pour la méthode ALS.\n",
    "  .addGrid(als.maxIter, Array(5, 20, 40, 60)) // Nombre d'itérations\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|rating|prediction|\n",
      "+------+----------+\n",
      "|     3| 3.3020227|\n",
      "|     4| 3.6534286|\n",
      "|     4| 2.8139138|\n",
      "|     5| 3.9554446|\n",
      "|     2| 3.0176945|\n",
      "+------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "RMSE = 0.8332195327770032"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_643b30fd3338\r\n",
       "model: org.apache.spark.ml.recommendation.ALSModel = ALSModel: uid=als_613446f4fea0, rank=10\r\n",
       "predictions: org.apache.spark.sql.DataFrame = [user_id: int, book_id: int ... 2 more fields]\r\n",
       "rmse: Double = 0.8332195327770032\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(als)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setTrainRatio(0.8)\n",
    "  .setParallelism(2)\n",
    "\n",
    "// Fitting du modèle\n",
    "val model = als.fit(training)\n",
    "\n",
    "// Prediction en test\n",
    "val predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"rating\", \"prediction\").show(5)\n",
    "\n",
    "// Calcule du RMSE sur le test\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(s\"RMSE = ${rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Générer le top 3 des livres à recommender pour chaque utilisateur**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|    148|[[3628, 4.344607]...|\n",
      "|    463|[[8946, 4.1403933...|\n",
      "|    471|[[8946, 4.3629804...|\n",
      "|    496|[[6089, 5.020456]...|\n",
      "|    833|[[8946, 4.76129],...|\n",
      "|   1088|[[8946, 4.9013343...|\n",
      "|   1238|[[1935, 4.62988],...|\n",
      "|   1342|[[8946, 4.612919]...|\n",
      "|   1580|[[3628, 4.6657906...|\n",
      "|   1591|[[3628, 4.700842]...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "userRecs: org.apache.spark.sql.DataFrame = [user_id: int, recommendations: array<struct<book_id:int,rating:float>>]\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userRecs = model.recommendForAllUsers(3)\n",
    "userRecs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Générer pour chaque livre, le top 3 des utilisateurs peuvent être intéressés**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|book_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|   1580|[[41383, 4.762112...|\n",
      "|   4900|[[51626, 4.818962...|\n",
      "|   5300|[[52237, 4.730690...|\n",
      "|   6620|[[3556, 5.032635]...|\n",
      "|   7240|[[9514, 5.3250403...|\n",
      "|   7340|[[32722, 4.886296...|\n",
      "|   7880|[[38076, 4.88214]...|\n",
      "|   9900|[[22895, 5.19854]...|\n",
      "|    471|[[44751, 4.813693...|\n",
      "|   1591|[[29106, 5.019598...|\n",
      "+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "booksRecs: org.apache.spark.sql.DataFrame = [book_id: int, recommendations: array<struct<user_id:int,rating:float>>]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val booksRecs = model.recommendForAllItems(3)\n",
    "booksRecs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Générer le top 3 des livres à recommander pour une liste d'utilisateurs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommandations pour user ID 1580 =>\n",
      "book_id, rating = [3628,4.6657906]\n",
      "book_id, rating = [9578,4.65385]\n",
      "book_id, rating = [8946,4.593843]\n",
      "Recommandations pour user ID 463 =>\n",
      "book_id, rating = [8946,4.1403933]\n",
      "book_id, rating = [2082,4.03369]\n",
      "book_id, rating = [4868,4.0255046]\n",
      "Recommandations pour user ID 1238 =>\n",
      "book_id, rating = [1935,4.62988]\n",
      "book_id, rating = [9094,4.3564425]\n",
      "book_id, rating = [8854,4.3408766]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: int]\r\n",
       "userSubsetRecs: org.apache.spark.sql.DataFrame = [user_id: int, recommendations: array<struct<book_id:int,rating:float>>]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val users = ratingsDF.select(als.getUserCol).distinct().limit(3)\n",
    "val userSubsetRecs = model.recommendForUserSubset(users, 3)\n",
    "userSubsetRecs.collect().foreach {\n",
    "      case Row(user_id: Int, recommendations : WrappedArray[_]) \n",
    "                => {\n",
    "                    println(s\"Recommandations pour user ID $user_id =>\") \n",
    "                    recommendations.asInstanceOf[Seq[Array[_]]].map(\n",
    "                       x => println(s\"book_id, rating = ${x}\"))\n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Générer le top 3 des utilisateurs qui peuvent être intéressés pour une liste de livre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Livre 471:\n",
      "Recommendations : =>\n",
      "user_id, rating = [44751,4.813693]\n",
      "user_id, rating = [44678,4.7945585]\n",
      "user_id, rating = [52237,4.7913785]\n",
      "Livre 2142:\n",
      "Recommendations : =>\n",
      "user_id, rating = [32574,5.0247736]\n",
      "user_id, rating = [27472,5.0161223]\n",
      "user_id, rating = [46749,5.002241]\n",
      "Livre 148:\n",
      "Recommendations : =>\n",
      "user_id, rating = [9514,4.829982]\n",
      "user_id, rating = [8377,4.7716002]\n",
      "user_id, rating = [29742,4.7560544]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "books: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [book_id: int]\r\n",
       "booksSubSetRecs: org.apache.spark.sql.DataFrame = [book_id: int, recommendations: array<struct<user_id:int,rating:float>>]\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val books = ratingsDF.select(als.getItemCol).distinct().limit(3)\n",
    "val booksSubSetRecs = model.recommendForItemSubset(books, 3)\n",
    "\n",
    "booksSubSetRecs.collect().foreach {\n",
    "      case Row(book_id: Int, recommendations : WrappedArray[_]) \n",
    "                => {\n",
    "                    println(s\"Livre $book_id:\") \n",
    "                    println(\"Recommendations : =>\")\n",
    "                   recommendations.asInstanceOf[Seq[Array[_]]].map(\n",
    "                       x => println(s\"user_id, rating = ${x}\"))\n",
    "                }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Références :**   \n",
    "\n",
    "[Documentation Spark](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html)      \n",
    "[WikiStat](https://github.com/wikistat/AI-Frameworks/blob/master/RecomendationSystem/pyspark.ipynb)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
